{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Semi-supervised EM\n",
    "\n",
    "Expectation Maximization (EM) is a classical algorithm for unsupervised learning (i.e., learning with hidden or latent variables). In this problem we will explore one of the ways in which EM algorithm can be adapted to the semi-supervised setting, where we have some labelled examples along with unlabelled examples.\n",
    "\n",
    "In the standard unsupervised setting, we have $m\\in\\mathbb{N}$ unlabelled examples $\\{x^{(1)},\\ldots,x^{(m)}\\}$. We wish to learn the parameters of $p(x,z;\\theta)$ from the data, but $z^{(i)}$'s are not observed. The classical EM algorithm is designed for this very purpose, where we maximize the intractable $p(x;\\theta)$ indirectly by iteratively performing the E-step and M-step, each time maximizing a tractable lower bound of $p(x;\\theta)$. Our objective can be concretely written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell_{unsup}(\\theta) \n",
    "& = \\sum_{i=1}^m\\log p(x^{(i)};\\theta)\\\\\n",
    "& = \\sum_{i=1}^m\\log \\sum_{z}p(x^{(i)},z^{(i)};\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "Now, we will attempt to construct an extension of EM to the semi-supervised setting. Let us suppose we have an additional $\\tilde{m}\\in\\mathbb{N}$ labelled examples $\\{(\\tilde{x}^{(1)},\\tilde{z}^{(1)}),\\ldots,(\\tilde{x}^{(\\tilde{m})},\\tilde{z}^{(\\tilde{m})})\\}$ where both $x$ and $z$ are observed. We want to simultaneously maximize the marginal likelihood of the parameters using the unlabelled examples, and full likelihood of the parameters using the labelled examples, by optimizing their weighted sum (with some hyperparameter $\\alpha$). More concretely, our semi-supervised objective $\\ell_{semi-unsup}(\\theta)$ can be written as:\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell_{sup}(\\theta) \n",
    "& = \\sum_{i=1}^{\\tilde{m}}\\log p(\\tilde{x}^{(i)},\\tilde{z}^{(i)};\\theta)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\ell_{semi-sup}(\\theta) \n",
    "& = \\ell_{unsup}(\\theta)  + \\alpha\\ell_{sup}(\\theta).\n",
    "\\end{align*}\n",
    "\n",
    "We can derive the EM steps for the semi-supervised setting using the same approach and steps as before. You are strongly encouraged to show to yourself (no need to include in the write-up) that we end up with:\n",
    "\n",
    "## E-step (semi-supervised)\n",
    "\n",
    "For each $i\\in\\{1,\\ldots,m\\}$, set\n",
    "\\begin{align*}\n",
    "Q_{i}^{(t)}(z^{(i)}) := p(z^{(i)}|x^{(i)};\\theta^{(t)}).\n",
    "\\end{align*}\n",
    "\n",
    "## M-step (semi-supervised)\n",
    "\n",
    "\n",
    "For each $i\\in\\{1,\\ldots,m\\}$, set\n",
    "\\begin{align*}\n",
    "\\theta^{(t+1)} =\\arg\\max_{\\theta}\\left[\\sum_{i=1}^m\\left(\\sum_{z^{(i)}}\n",
    "Q_{i}^{(t)}(z^{(i)})\\log \\frac{p(x^{(i)},z^{(i)};\\theta)}{Q_{i}^{(t)}(z^{(i)})}\\right) +\\left(\\sum_{i=1}^{\\tilde{m}}\\log p(\\tilde{x}^{(i)},\\tilde{z}^{(i)};\\theta)\\right) \\right].\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [5 points] Convergence. \n",
    "\n",
    "First we will show that this algorithm eventually converges. In order to prove this, it is sufficient to show that our semisupervised objective $\\ell_{semi-sup}(\\theta)$ monotonically increases with each iteration of E and M step. Specifically, let $\\theta^{(t)}$ be the parameters obtained at the end of $t$ EM-steps. Show that \n",
    "\n",
    "\\begin{align*}\n",
    "\\ell_{semi-sup}(\\theta^{(t+1)})\\geq \\ell_{semi-sup}(\\theta^{(t)}). \n",
    "\\end{align*}\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\ell_{semi-sup}(\\theta^{(t+1)}) \n",
    "& = \\ell_{unsup}(\\theta^{(t+1)})  + \\alpha\\ell_{sup}(\\theta^{(t+1)})\\\\\n",
    "& = \\sum_{i=1}^m\\log \\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\\theta^{(t+1)}) + \\sum_{i=1}^{\\tilde{m}}\\log p(\\tilde{x}^{(i)},\\tilde{z}^{(i)};\\theta^{(t+1)})\\\\\n",
    "& = \\sum_{i=1}^m\\log \\sum_{z^{(i)}}Q_{i}^{(t)}(z^{(i)})\\frac{p(x^{(i)},z^{(i)};\\theta^{(t+1)})}{Q_{i}^{(t)}(z^{(i)})} + \\sum_{i=1}^{\\tilde{m}}\\log p(\\tilde{x}^{(i)},\\tilde{z}^{(i)};\\theta^{(t+1)})\\\\\n",
    "& \\geq \\sum_{i=1}^m \\sum_{z^{(i)}}Q_{i}^{(t)}(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)};\\theta^{(t+1)})}{Q_{i}^{(t)}(z^{(i)})} + \\sum_{i=1}^{\\tilde{m}}\\log p(\\tilde{x}^{(i)},\\tilde{z}^{(i)};\\theta^{(t+1)})\\\\\n",
    "& \\geq \\sum_{i=1}^m \\sum_{z^{(i)}}Q_{i}^{(t)}(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)};\\theta^{(t)})}{Q_{i}^{(t)}(z^{(i)})} + \\sum_{i=1}^{\\tilde{m}}\\log p(\\tilde{x}^{(i)},\\tilde{z}^{(i)};\\theta^{(t)})\\\\\n",
    "& = \\ell_{semi-sup}(\\theta^{(t)}),\n",
    "\\end{align*}\n",
    "\n",
    "where the first inequality comes from the fact that $\\log(.)$ is a concave function, the second one is true because of how we have chosen $\\theta^{(t+1)}$, and the last equality holds since, in E-step, we set $Q$'s in a way that this equality gets true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised GMM\n",
    "\n",
    "Now we will revisit the Gaussian Mixture Model (GMM), to apply our semi-supervised EM algorithm. Let us consider a scenario where data is generated from $k\\in\\mathbb{N}$ Gaussian distributions, with unknown means $\\mu_j\\in \\mathbb{R}^d$ and covariances $\\Sigma_j\\in \\mathbb{S}^d_+$ where $j\\in\\{1,\\ldots,k\\}$. We have $m$ data points \n",
    "$x^{(i)}\\in \\mathbb{R}^d$, $i\\in\\{1,\\ldots,m\\}$, and each data point has a corresponding latent (hidden/unknown) variable $z^{(i)}\\in \\{1,\\ldots,k\\}$ indicating which distribution $x^{(i)}$ belongs to. Specifically, \n",
    "$z^{(i)}\\sim {\\rm Multinomial}(\\phi)$, such that $\\sum_{j=1}^k\\phi_j =1$ and $\\phi_j\\geq 0$ for all $j$ and \n",
    "$x^{(i)}|z^{(i)}\\sim \\mathcal{N}(\\mu_{z^{(i)}},\\Sigma_{z^{(i)}})$\n",
    "i.i.d. So, $\\mu, \\Sigma$, and $\\phi$ are the model parameters.\n",
    "\n",
    "We also have an additional $\\tilde{m}$ data points $\\tilde{x}^{(i)}\\in \\mathbb{R}^d$, $i\\in\\{1,\\ldots,\\tilde{m}\\}$, and an associated observed variable $\\tilde{z}\\in\\{1,\\ldots,k\\}$ indicating the distribution $x^{(i)}$ belong to.\n",
    "Note that $\\tilde{z}^{(i)}$ are known constants (in contrast to $z^{(i)}$ which are unknown random variables). As before, we assume $\\tilde{x}^{(i)}|\\tilde{z}^{(i)}\\sim \\mathcal{N}(\\mu_{\\tilde{z}^{(i)}},\\Sigma_{\\tilde{z}^{(i)}})$ i.i.d.\n",
    "\n",
    "In summary we have $m+\\tilde{m}$ examples, of which $m$ are unlabelled data points $x$'s with unobserved $z$'s and $m$ are labelled data points $\\tilde{x}^{(i)}$ with corresponding observed labels $\\tilde{z}^{(i)}$.\n",
    "The traditional EM algorithm is designed to take only the $m$ unlabelled examples as input, and learn the model parameters $\\mu, \\Sigma$, and $\\phi$.\n",
    "\n",
    "Our task now will be to apply the semi-supervised EM algorithm to GMMs in order to leverage the additional $\\tilde{m}$ labelled examples, and come up with semi-supervised E-step and M-step update rules specific to GMMs. Whenever required, you can cite the lecture notes for derivations and steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [5 points] Semi-supervised E-Step. \n",
    "\n",
    "Clearly state which are all the latent variables that need to be re-estimated in the E-step. Derive the E-step to re-estimate all the stated latent variables. Your final E-step expression must only involve $x,z,\\mu, \\Sigma,\\phi$ and universal constants.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent variables are $z^{(i)}$ for $i\\in\\{1,\\ldots,m\\}$. \n",
    "\n",
    "\\begin{align*}\n",
    "\\ell_{semi-sup}(\\theta^{(t)}) \n",
    "& = \\ell_{unsup}(\\theta^{(t)})  + \\alpha \\ell_{sup}(\\theta^{(t)})\\\\\n",
    "& = \\sum_{i=1}^m\\log \\sum_{z}p(x^{(i)},z^{(i)};\\theta^{(t)}) + \\sum_{i=1}^{\\tilde{m}}\\log p(\\tilde{x}^{(i)},\\tilde{z}^{(i)};\\theta^{(t)})\\\\\n",
    "& = \\sum_{i=1}^m\\log \\sum_{z}Q_{i}^{(t)}(z^{(i)})\\frac{p(x^{(i)},z^{(i)};\\theta^{(t+1)})}{Q_{i}^{(t)}(z^{(i)})} + \\sum_{i=1}^{\\tilde{m}}\\log p(\\tilde{x}^{(i)},\\tilde{z}^{(i)};\\theta^{(t)})\\\\\n",
    "& \\geq \\sum_{i=1}^m \\sum_{z}Q_{i}^{(t)}(z^{(i)})\\log\\frac{p(x^{(i)},z^{(i)};\\theta^{(t)})}{Q_{i}^{(t)}(z^{(i)})} + \\sum_{i=1}^{\\tilde{m}}\\log p(\\tilde{x}^{(i)},\\tilde{z}^{(i)};\\theta^{(t)})\\\\\n",
    "& = J(Q^{(t)},\\theta^{(t)}).\n",
    "\\end{align*}\n",
    "\n",
    "Note that we first start with a random $\\theta^{(0)}$ and then we are looking for a $Q^{(t)}$ such that $\\ell_{semi-sup}(\\theta^{(t)})=J(Q^{(t)},\\theta^{(t)})$. \n",
    "Follwoing the same approch as in Lectute note, we come up with the follwoing update rule:\n",
    "\n",
    "\\begin{align*}\n",
    "{\\omega_j^{(i)}}^{(t)} \n",
    "& = Q_i^{(t)}(z=j) \\\\ \\\\\n",
    "& = p\\left(z=j|x^{(i)};\\phi^{(t)}, \\mu^{(t)}, \\Sigma^{(t)}\\right)\\\\ \\\\\n",
    "& = \\frac{p\\left(x^{(i)}|z=j;\\mu^{(t)}, \\Sigma^{(t)}\\right)p\\left(z=j;\\phi^{(t)}\\right)}{\\sum_{l=1}^k p\\left(x^{(i)}|z=l;\\mu^{(t)}, \\Sigma^{(t)}\\right)p\\left(z=l;\\phi^{(t)}\\right)}\\\\ \\\\\n",
    "& = \\frac{\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma_j^{(t)}|^{\\frac{1}{2}}}\\exp\\left\\{-\\frac{1}{2}\\left(x^{(i)}-\\mu_j^{(t)}\\right)^T{\\Sigma^{(t)}_j}^{-1}\\left(x^{(i)}-\\mu_j^{(t)}\\right)\\right\\}\\phi^{(t)}_j}{\\sum_{l=1}^k \n",
    "\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma_l^{(t)}|^{\\frac{1}{2}}}\\exp\\left\\{-\\frac{1}{2}\\left(x^{(i)}-\\mu_l^{(t)}\\right)^T{\\Sigma^{(t)}_l}^{-1}\\left(x^{(i)}-\\mu_l^{(t)}\\right)\\right\\}\\phi^{(t)}_l}.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [5 points] Semi-supervised M-Step. \n",
    "\n",
    "Clearly state which are all the parameters that need to be re-estimated in the M-step. Derive the M-step to reestimate all the stated parameters. Specifically, derive closed form expressions for the parameter update rules for $\\mu^{(t+1)}, \\Sigma^{(t+1)}$, and $\\phi^{(t+1)}$ based on the semi-supervised objective.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The parameters that need to be re-estimated in the M-step are  $\\mu_j, \\Sigma_j$, and $\\phi_j$ for each $j\\in\\{1,\\ldots,k\\}$. To derive the update rule, we need to solve the following optimization problem. \n",
    "\\begin{align*}\n",
    "\\mu^{(t+1)},\\Sigma^{(t+1)},\\phi^{(t+1)} \n",
    "& = {\\arg\\max}_{\\mu,\\Sigma,\\phi}\\sum_{i=1}^m\\left\\{ \\alpha\\sum_{j=1}^k{\\omega^{(i)}_j}^{(t)}\\log\\frac{p(x^{(i)},z^{(i)}=j;\\mu,\\Sigma,\\phi)}{{\\omega^{(i)}_j}^{(t)}} + \\alpha\\sum_{i=1}^{\\tilde{m}}\\log p(\\tilde{x}^{(i)},\\tilde{z}^{(i)};\\mu,\\Sigma,\\phi)\\right\\}\\\\ \\\\\n",
    "& = {\\arg\\max}_{\\mu,\\Sigma,\\phi}\n",
    "\\left\\{\\sum_{i=1}^m \\sum_{j=1}^k{\\omega^{(i)}_j}^{(t)}\\log\\frac{\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma_j|^{\\frac{1}{2}}}\\exp\\left\\{-\\frac{1}{2}\\left(x^{(i)}-\\mu_j\\right)^T{\\Sigma_j}^{-1}\\left(x^{(i)}-\\mu_j\\right)\\right\\}p\\left(z^{(i)}=j|\\phi\\right)}{{\\omega^{(i)}_j}^{(t)}} + \\alpha\\sum_{i=1}^{\\tilde{m}}\\log \\left(\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma_{\\tilde{z}^{(i)}}^{(t)}|^{\\frac{1}{2}}}\\exp\\left\\{-\\frac{1}{2}\\left(\\tilde{x}^{(i)}-\\mu_{\\tilde{z}^{(i)}}\\right)^T{\\Sigma_{\\tilde{z}^{(i)}}}^{-1}\\left(\\tilde{x}^{(i)}-\\mu_{\\tilde{z}^{(i)}}\\right)\\right\\}p\\left(\\tilde{z}^{(i)};\\phi\\right)\\right)\\right\\} \\\\ \\\\ \n",
    "& = {\\arg\\max}_{\\mu,\\Sigma,\\phi}\n",
    "\\left\\{\\sum_{i=1}^m \\sum_{j=1}^k\\left(-\\frac{1}{2}{\\omega^{(i)}_j}^{(t)}\\log|\\Sigma_j|-\\frac{1}{2}{\\omega^{(i)}_j}^{(t)}\\left(x^{(i)}-\\mu_j\\right)^T{\\Sigma_j}^{-1}\\left(x^{(i)}-\\mu_j\\right)+{\\omega^{(i)}_j}^{(t)}\\log \\phi_j\\right) + \n",
    "\\alpha\\sum_{i=1}^{\\tilde{m}}\\left(-{\\frac{1}{2}}\\log |\\Sigma_{\\tilde{z}^{(i)}}|-\\frac{1}{2}\\left(\\tilde{x}^{(i)}-\\mu_{\\tilde{z}^{(i)}}\\right)^T{\\Sigma_{\\tilde{z}^{(i)}}}^{-1}\\left(\\tilde{x}^{(i)}-\\mu_{\\tilde{z}^{(i)}}\\right)+\\log \\phi_{\\tilde{z}^{(i)}}\\right) \\right\\}\n",
    "\\end{align*}\n",
    "subjet to the constraints $\\sum_{j=1}^k\\phi_j=1$ and $\\phi_j\\geq 0$ for each $j\\in\\{1,\\ldots,k\\}$.\n",
    "\n",
    "For simplicity of notation, call this objective function $\\mathcal{J}(\\mu,\\Sigma,\\phi)$. Using the method of Lagrange multipliers, we have \n",
    "\n",
    "\\begin{align*}\n",
    "\\phi^{(t+1)}_j & = {\\arg\\max}_{\\phi,\\lambda}\\mathcal{L}(\\phi,\\lambda)\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\phi,\\lambda) = \\sum_{i=1}^m \\sum_{j=1}^k {\\omega^{(i)}_j}^{(t)}\\log \\phi_j + \\alpha\\sum_{i=1}^{\\tilde{m}} \\log \\phi_{\\tilde{z}^{(i)}} - \\lambda(\\sum_{j=1}^k\\phi_j -1).\n",
    "\\end{align*}\n",
    "\n",
    "Thus, setting $\\nabla_{\\phi_j}\\mathcal{L}(\\phi,\\lambda) = 0$ for each $j\\in\\{1,\\ldots,k\\}$ and $\\nabla_{\\lambda}\\mathcal{L}(\\phi,\\lambda) = 0$, we have \n",
    "\n",
    "\\begin{align*} \n",
    "&\\sum_{i=1}^m{\\omega^{(i)}_j}^{(t)} +\\alpha\\sum_{i=1}^{\\tilde{m}}1_{\\{\\tilde{z}^{(i)} = j\\}} =\\phi_j\\lambda\\\\\n",
    "&\\sum_{j=1}^k\\phi_j = 1.\n",
    "\\end{align*}\n",
    "\n",
    "Combining these two equalities, we find \n",
    "\\begin{align*} \n",
    "\\lambda & = \\sum_{i=1}^m\\sum_{j=1}^k{\\omega^{(i)}_j}^{(t)} +\\alpha\\sum_{i=1}^{\\tilde{m}}\\sum_{j=1}^k1_{\\{\\tilde{z}^{(i)} = j\\}}\\\\\n",
    "& = m +\\alpha\\tilde{m}.\n",
    "\\end{align*}\n",
    "Consequently, \n",
    "\\begin{equation*} \n",
    "\\phi_j^{(t+1)} =  \\frac{\\sum_{i=1}^m{\\omega^{(i)}_j}^{(t)} +\\alpha\\sum_{i=1}^{\\tilde{m}}1_{\\{\\tilde{z}^{(i)} = j\\}}}{m +\\alpha\\tilde{m}}.\n",
    "\\label{1} \\tag{1}\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, by setting the derivative of $\\mathcal{J}(\\mu,\\Sigma,\\phi)$ with respect to $\\mu_j$'s and $\\Sigma_j$'s to zero, we obtain:\n",
    "\n",
    "\\begin{align*}\n",
    "\\nabla_{\\mu_j}\\mathcal{J}(\\mu,\\Sigma,\\phi)\n",
    "& = {\\Sigma_j}^{-1}\\left(\\sum_{i=1}^m {\\omega^{(i)}_j}^{(t)}\\left(x^{(i)}-\\mu_j\\right) + \\alpha\\sum_{i=1}^{\\tilde{m}}1_{\\{\\tilde{z}^{(i)} = j\\}}\\left(\\tilde{x}^{(i)}-\\mu_{j}\\right)\\right)\\\\\n",
    "& = 0\n",
    "\\end{align*}\n",
    "which implies \n",
    " \n",
    "\\begin{equation*} \n",
    "\\mu_j^{(t+1)} =  \\frac{\\sum_{i=1}^m {\\omega^{(i)}_j}^{(t)}x^{(i)} + \\alpha\\sum_{i=1}^{\\tilde{m}}1_{\\{\\tilde{z}^{(i)} = j\\}}\\tilde{x}^{(i)}}{\\sum_{i=1}^m {\\omega^{(i)}_j}^{(t)} + \\alpha\\sum_{i=1}^{\\tilde{m}}1_{\\{\\tilde{z}^{(i)} = j\\}}}.\n",
    "\\label{2} \\tag{2}\n",
    "\\end{equation*}\n",
    "\n",
    "And \n",
    "\\begin{align*}\n",
    "\\nabla_{\\Sigma_j}\\mathcal{J}(\\mu^{t+1},\\Sigma,\\phi^{t+1})\n",
    "& = -\\frac{1}{2}{\\Sigma_j}^{-1}\\sum_{i=1}^m{\\omega^{(i)}_j}^{(t)} + \\frac{1}{2}{\\Sigma_j}^{-1}\\left(\\sum_{i=1}^m{\\omega^{(i)}_j}^{(t)}\\left(x^{(i)}-\\mu_j^{(t+1)}\\right)\\left(x^{(i)}-\\mu_j^{(t+1)}\\right)^T\\right){\\Sigma^{(t)}_j}^{-1}\\\\\n",
    "& -\\frac{1}{2}\\alpha{\\Sigma_j}^{-1}\\sum_{i=1}^{\\tilde{m}}1_{\\{\\tilde{z}^{(i)}=j\\}} + \\frac{1}{2}{\\Sigma_j}^{-1}\\left(\\sum_{i=1}^{\\tilde{m}}1_{\\{\\tilde{z}^{(i)}=j\\}}\n",
    "\\left(\\tilde{x}^{(i)}-\\mu_j^{(t+1)}\\right)\\left(\\tilde{x}^{(i)}-\\mu_j^{(t+1)}\\right)^T\\right){\\Sigma_j}^{-1}\\\\\n",
    "& = 0\n",
    "\\end{align*}\n",
    "\n",
    "which concludes\n",
    "\n",
    "\\begin{equation*}\n",
    "\\Sigma_j^{(t+1)}\n",
    "= \\frac{\\sum_{i=1}^m{\\omega^{(i)}_j}^{(t)}\\left(x^{(i)}-\\mu_j^{(t+1)}\\right)\\left(x^{(i)}-\\mu_j^{(t+1)}\\right)^T + \\sum_{i=1}^{\\tilde{m}}1_{\\{\\tilde{z}^{(i)}=j\\}}\n",
    "\\left(\\tilde{x}^{(i)}-\\mu_j^{(t+1)}\\right)\\left(\\tilde{x}^{(i)}-\\mu_j^{(t+1)}\\right)^T}{\\sum_{i=1}^m{\\omega^{(i)}_j}^{(t)} + \\sum_{i=1}^{\\tilde{m}}1_{\\{\\tilde{z}^{(i)}=j\\}}}.\n",
    "\\label{3} \\tag{3}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) [5 points] [Coding Problem] Classical (Unsupervised) EM Implementation. \n",
    "\n",
    "For this sub-question, we are only going to consider the $m$ unlabelled examples. Follow the instructions in `src/p03_gmm.py` to implement the traditional EM algorithm, and run it on the unlabelled data-set until convergence.\n",
    "\n",
    "Run three trials and use the provided plotting function to construct a scatter plot of the resulting assignments to clusters (one plot for each trial). Your plot should indicate cluster assignments with colors they got assigned to (i.e., the cluster which had the highest probability in the final E-step).\n",
    "\n",
    "__Note:__ You only need to submit the three plots in your write-up. Your code will not be autograded.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from src.p03_gmm import plot_gmm_preds, load_gmm_dataset\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_w(x, mu, sigma, phi):\n",
    "    K = len(mu)\n",
    "    m,n = x.shape\n",
    "    w = np.zeros((m,K))\n",
    "    for j in range(K):\n",
    "        w[:,j] = p_x_given_z(x,j,mu[j],sigma[j])*phi[j]\n",
    "        #sigma_j_inv = np.linalg.inv(sigma[j])\n",
    "        #sigma_j_det = np.linalg.det(sigma_j_inv)\n",
    "        #temp = np.exp(-.5 *np.sum(((x-mu[j])@sigma_j_inv)*(x-mu[j]) , axis = 1))\n",
    "        #w[:,j] = (2*np.pi)**(n/2)*np.sqrt(sigma_j_det)* phi[j] * temp\n",
    "    return w/np.sum(w, axis = 1, keepdims=True)\n",
    "\n",
    "def M_step(x, mu, sigma, phi, w):\n",
    "    # w.shape = (m,K)\n",
    "    # x.shape = (m,n)\n",
    "    phi = np.mean(w, axis = 0)\n",
    "    mu = list((w.T@x)/np.sum(w, axis = 0).reshape(-1,1))\n",
    "    for j in range(K):\n",
    "        sigma[j] = (((x-mu[j])*w[:,j].reshape(-1,1)).T@(x-mu[j]))/np.sum(w[:,j])\n",
    "    return mu, sigma, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_x_given_z(x,j,mu_j,sigma_j):\n",
    "    m,n = x.shape\n",
    "    sigma_j_inv = np.linalg.inv(sigma_j)\n",
    "    sigma_j_det = np.linalg.det(sigma_j_inv)\n",
    "    temp = np.exp(-.5 *np.sum(((x-mu_j)@sigma_j_inv)*(x-mu_j), axis = 1))\n",
    "    return np.sqrt(sigma_j_det)* temp/((2*np.pi)**(n/2))\n",
    "\n",
    "def log_likelihood(x,K,mu,sigma,phi, x_tilde=None, z_tilde = None):\n",
    "    m,_ = x.shape\n",
    "    p = np.zeros((m,))\n",
    "    for z in range(K):\n",
    "        p = p + p_x_given_z(x,z,mu[z],sigma[z])*phi[z]\n",
    "    p = np.log(p)\n",
    "    \n",
    "    if x_tilde is not None:\n",
    "        q = np.zeros((m,))\n",
    "        for z in range(K):\n",
    "            x_0 = x_tilde[z_tilde.squeez() == z]\n",
    "            q = q + p_x_given_z(x_0,z,mu[z],sigma[z])*phi[z]\n",
    "            \n",
    "        p = p + np.log(q)\n",
    "    \n",
    "    return np.sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_COLORS = ['red', 'green', 'blue', 'orange']  # Colors for your plots\n",
    "K = 4         # Number of Gaussians in the mixture model\n",
    "NUM_TRIALS = 3  # Number of trials to run (can be adjusted for debugging)\n",
    "UNLABELED = -1  # Cluster label for unlabeled data points (do not change)\n",
    "\n",
    "log_likelihood_list_1 = []\n",
    "log_likelihood_list_2 = []\n",
    "\n",
    "def main(is_semi_supervised, trial_num):\n",
    "    \"\"\"Problem 3: EM for Gaussian Mixture Models (unsupervised and semi-supervised)\"\"\"\n",
    "    print('Running {} EM algorithm...'\n",
    "          .format('semi-supervised' if is_semi_supervised else 'unsupervised'))\n",
    "\n",
    "    # Load dataset\n",
    "    train_path = os.path.join('data', 'ds3_train.csv')\n",
    "    x, z = load_gmm_dataset(train_path)\n",
    "    x_tilde = None\n",
    "\n",
    "    if is_semi_supervised:\n",
    "        # Split into labeled and unlabeled examples\n",
    "        labeled_idxs = (z != UNLABELED).squeeze()\n",
    "        x_tilde = x[labeled_idxs, :]   # Labeled examples\n",
    "        z = z[labeled_idxs, :]         # Corresponding labels\n",
    "        x = x[~labeled_idxs, :]        # Unlabeled examples\n",
    "\n",
    "    # *** START CODE HERE ***\n",
    "    # (1) Initialize mu and sigma by splitting the m data points uniformly at random\n",
    "    # into K groups, then calculating the sample mean and covariance for each group\n",
    "    m, n = x.shape \n",
    "    P = np.random.randint(K, size = m)\n",
    "    mu = [np.mean(x[P == i], axis = 0) for i in range(K)] # each entry is of shape (1,n)\n",
    "    sigma =[np.cov(x[P == i].T) for i in range(K)] #each entry is of shape (n,n)\n",
    "    \n",
    "    # (2) Initialize phi to place equal probability on each Gaussian\n",
    "    # phi should be a numpy array of shape (K,)\n",
    "    \n",
    "    phi = 1/K * np.ones((K,))\n",
    "    \n",
    "    # (3) Initialize the w values to place equal probability on each Gaussian\n",
    "    # w should be a numpy array of shape (m, K)\n",
    "    \n",
    "    w = 1/K * np.ones((m,K))\n",
    "    \n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "    if is_semi_supervised:\n",
    "        w = run_semi_supervised_em(x, x_tilde, z, w, phi, mu, sigma, trial_num)\n",
    "    else:\n",
    "        w = run_em(x, w, phi, mu, sigma, trial_num)\n",
    "\n",
    "    # Plot your predictions\n",
    "    z_pred = np.zeros(m)\n",
    "    if w is not None:  # Just a placeholder for the starter code\n",
    "        for i in range(m):\n",
    "            z_pred[i] = np.argmax(w[i])\n",
    "    plot_gmm_preds(x, z_pred, is_semi_supervised, plot_id=trial_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_em(x, w, phi, mu, sigma, trial_num=None):\n",
    "    \"\"\"Problem 3(d): EM Algorithm (unsupervised).\n",
    "\n",
    "    See inline comments for instructions.\n",
    "\n",
    "    Args:\n",
    "        x: Design matrix of shape (m, n).\n",
    "        w: Initial weight matrix of shape (m, k).\n",
    "        phi: Initial mixture prior, of shape (k,).\n",
    "        mu: Initial cluster means, list of k arrays of shape (n,).\n",
    "        sigma: Initial cluster covariances, list of k arrays of shape (n, n).\n",
    "\n",
    "    Returns:\n",
    "        Updated weight matrix of shape (m, k) resulting from EM algorithm.\n",
    "        More specifically, w[i, j] should contain the probability of\n",
    "        example x^(i) belonging to the j-th Gaussian in the mixture.\n",
    "    \"\"\"\n",
    "    # No need to change any of these parameters\n",
    "    eps = 1e-3  # Convergence threshold\n",
    "    max_iter = 1000\n",
    "\n",
    "    # Stop when the absolute change in log-likelihood is < eps\n",
    "    # See below for explanation of the convergence criterion\n",
    "    it = 0\n",
    "    ll = prev_ll = None\n",
    "    LL = []\n",
    "    while it < max_iter and (prev_ll is None or np.abs(ll - prev_ll) >= eps):\n",
    "        pass  # Just a placeholder for the starter code\n",
    "        # *** START CODE HERE\n",
    "        # (1) E-step: Update your estimates in w\n",
    "        \n",
    "        w = update_w(x, mu, sigma, phi)\n",
    "        \n",
    "        # (2) M-step: Update the model parameters phi, mu, and sigma\n",
    "        \n",
    "        mu, sigma, phi = M_step(x, mu, sigma, phi, w)\n",
    "        \n",
    "        # (3) Compute the log-likelihood of the data to check for convergence.\n",
    "        # By log-likelihood, we mean `ll = sum_x[log(sum_z[p(x|z) * p(z)])]`.\n",
    "        # We define convergence by the first iteration where abs(ll - prev_ll) < eps.\n",
    "        # Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.\n",
    "        prev_ll = ll\n",
    "        ll = log_likelihood(x,K,mu,sigma,phi)\n",
    "        LL.append(ll)\n",
    "        it+=1\n",
    "        #print(ll)\n",
    "        # *** END CODE HERE ***\n",
    "    \n",
    "    log_likelihood_list_1.append(LL)\n",
    "    print('converged after {} iterations.'.format(it))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running unsupervised EM algorithm...\n",
      "converged after 165 iterations.\n",
      "Running unsupervised EM algorithm...\n",
      "converged after 169 iterations.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(229)\n",
    "    # Run NUM_TRIALS trials to see how different initializations\n",
    "    # affect the final predictions with and without supervision\n",
    "    for t in range(NUM_TRIALS):\n",
    "        main(is_semi_supervised=False, trial_num=t)\n",
    "\n",
    "        # *** START CODE HERE ***\n",
    "        # Once you've implemented the semi-supervised version,\n",
    "        # uncomment the following line.\n",
    "        # You do not need to add any other lines in this code block.\n",
    "        # main(with_supervision=True, trial_num=t)\n",
    "        # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('log-likelihood in supervised GMM Predictions')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('log-likelihood value')\n",
    "for k in range(NUM_TRIALS):\n",
    "    plt.plot(log_likelihood_list_1[k], label = 'Trial number {} '.format(k))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) [7 points] [Coding Problem] Semi-supervised EM Implementation. \n",
    "\n",
    "Now we will consider both the labelled and unlabelled examples (a total of $m+\\tilde{m}$ ), with $5$ labelled examples per cluster. We have provided starter code for splitting the dataset into a matrices $x$ of labelled examples and $x_{tilde}$ of unlabelled examples. Add to your code in `src/p03_gmm.py` to implement the modified EM algorithm, and run it on the dataset until convergence.\n",
    "\n",
    "Create a plot for each trial, as done in the previous sub-question.\n",
    "\n",
    "__Note:__ You only need to submit the three plots in your write-up. Your code will not be\n",
    "autograded.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_step_semi(x, w, x_tilde, z_tilde, mu, sigma, phi, alpha):\n",
    "    # w.shape = (m,K)\n",
    "    # x.shape = (m,n)\n",
    "    _, K = w.shape\n",
    "    m, _ = x.shape\n",
    "    m_tilde, _ = x_tilde.shape\n",
    "\n",
    "    w_tilde = np.zeros((m_tilde, K))\n",
    "    for i in range(K):\n",
    "        z_tilde = z_tilde.squeeze()\n",
    "        w_tilde[:,i] = (z_tilde == i)\n",
    "        \n",
    "    #print(w_tilde)\n",
    "    #print(z_tilde)\n",
    "    \n",
    "    phi = (np.sum(w, axis = 0) + alpha * np.sum(w_tilde, axis = 0))/(m+alpha*m_tilde)\n",
    "    \n",
    "    temp = (w.T@x + alpha * w_tilde.T@x_tilde)/(np.sum(w, axis = 0).reshape(-1,1) + alpha* np.sum(w_tilde, axis = 0).reshape(-1,1))\n",
    "    mu = list(temp)\n",
    "    \n",
    "    for j in range(K):\n",
    "        temp = ((x-mu[j])*w[:,j].reshape(-1,1)).T@(x-mu[j])+alpha* ((x_tilde-mu[j])*w_tilde[:,j].reshape(-1,1)).T@(x_tilde-mu[j])\n",
    "        sigma[j] = (temp)/(np.sum(w[:,j])+ alpha*np.sum(w_tilde[:,j]))\n",
    "    return mu, sigma, phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_semi_supervised_em(x, x_tilde, z, w, phi, mu, sigma,  trial_num):\n",
    "    \"\"\"Problem 3(e): Semi-Supervised EM Algorithm.\n",
    "\n",
    "    See inline comments for instructions.\n",
    "\n",
    "    Args:\n",
    "        x: Design matrix of unlabeled examples of shape (m, n).\n",
    "        x_tilde: Design matrix of labeled examples of shape (m_tilde, n).\n",
    "        z: Array of labels of shape (m_tilde, 1).\n",
    "        w: Initial weight matrix of shape (m, k).\n",
    "        phi: Initial mixture prior, of shape (k,).\n",
    "        mu: Initial cluster means, list of k arrays of shape (n,).\n",
    "        sigma: Initial cluster covariances, list of k arrays of shape (n, n).\n",
    "\n",
    "    Returns:\n",
    "        Updated weight matrix of shape (m, k) resulting from semi-supervised EM algorithm.\n",
    "        More specifically, w[i, j] should contain the probability of\n",
    "        example x^(i) belonging to the j-th Gaussian in the mixture.\n",
    "    \"\"\"\n",
    "    # No need to change any of these parameters\n",
    "    alpha = 20.  # Weight for the labeled examples\n",
    "    eps = 1e-3   # Convergence threshold\n",
    "    max_iter = 100\n",
    "\n",
    "    # Stop when the absolute change in log-likelihood is < eps\n",
    "    # See below for explanation of the convergence criterion\n",
    "    it = 0\n",
    "    ll = prev_ll = None\n",
    "    LL = []\n",
    "    while it < max_iter and (prev_ll is None or np.abs(ll - prev_ll) >= eps):\n",
    "        pass  # Just a placeholder for the starter code\n",
    "        # *** START CODE HERE ***\n",
    "        # (1) E-step: Update your estimates in w\n",
    "        \n",
    "        w = update_w(x, mu, sigma, phi)\n",
    "        \n",
    "        # (2) M-step: Update the model parameters phi, mu, and sigma\n",
    "        \n",
    "        mu, sigma, phi = M_step_semi(x, w, x_tilde, z, mu, sigma, phi, alpha)\n",
    "        \n",
    "        # (3) Compute the log-likelihood of the data to check for convergence.\n",
    "        # Hint: Make sure to include alpha in your calculation of ll.\n",
    "        # Hint: For debugging, recall part (a). We showed that ll should be monotonically increasing.\n",
    "        prev_ll = ll\n",
    "        ll = log_likelihood(x,K,mu,sigma,phi)\n",
    "        \n",
    "        LL.append(ll)\n",
    "        \n",
    "        it+=1\n",
    "        # *** END CODE HERE ***\n",
    "    log_likelihood_list_2.append(LL)\n",
    "    print('converged after {} iterations.'.format(it))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    np.random.seed(220)\n",
    "    # Run NUM_TRIALS trials to see how different initializations\n",
    "    # affect the final predictions with and without supervision\n",
    "    for t in range(NUM_TRIALS):\n",
    "        #main(is_semi_supervised=False, trial_num=t)\n",
    "\n",
    "        # *** START CODE HERE ***\n",
    "        # Once you've implemented the semi-supervised version,\n",
    "        # uncomment the following line.\n",
    "        # You do not need to add any other lines in this code block.\n",
    "        main(is_semi_supervised=True, trial_num=t)\n",
    "        # *** END CODE HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.title('log-likelihood in supervised GMM Predictions')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('log-likelihood value')\n",
    "for k in range(NUM_TRIALS):\n",
    "    plt.plot(log_likelihood_list_2[k], label = 'Trial number {} '.format(k))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f) [3 points] Comparison of Unsupervised and Semi-supervised EM. \n",
    "\n",
    "Briefly describe the differences you saw in unsupervised vs. semi-supervised EM for each of the following: \n",
    "\n",
    "> i. Number of iterations taken to converge.\n",
    "\n",
    "> ii. Stability (i.e., how much did assignments change with different random initializations?) \n",
    "\n",
    ">iii. Overall quality of assignments.\n",
    "\n",
    "\n",
    "__Note:__ The dataset was sampled from a mixture of three low-variance Gaussian distributions, and a fourth, high-variance Gaussian distribution. This should be useful in determining the overall quality of the assignments that were found by the two algorithms.\n",
    "\n",
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> i. Number of iteration taken to converge in semi-supervised EM is extremely less than that in unsupervised EM.\n",
    "\n",
    ">ii. In unsupervised EM, with different random initializations we have different assignments, specially in dealing with high variance distribution, but in semi-supervised EM the assignments are exactly the same even with different random initializations.\n",
    "\n",
    ">iii. Although, both supervised EM did its job not bad, the semi-supervised EM is much more robust with different initializations and also working with high variance distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
